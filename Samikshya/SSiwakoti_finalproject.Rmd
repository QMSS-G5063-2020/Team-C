---
title: "Data Visualization Final"
author: "Samikshya Siwakoti"
date: "4/19/2020"
output:
  html_document:
    highlight: textmate
    theme: spacelab
    toc: yes
  html_notebook:
    toc: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
always_allow_html: yes
---


```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(stringi)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(corpus)
library(purrr)
library(gower)
library(lme4)
library(multcomp)
library(mvtnorm)
library(corpustools)
```

Input the text files

```{r, messages=FALSE, warning=FALSE}
text <- readLines("~/Desktop/dataviz/final_project/all_nations.txt")
text1 <- readLines("~/Desktop/dataviz/final_project/syria_v1.txt")
text2 <- readLines("~/Desktop/dataviz/final_project/venezuela_v1.txt")
text3 <- readLines("~/Desktop/dataviz/final_project/ssudan_v1.txt")
```

Converting to corpus and cleaning

```{r}
cloud <- Corpus(VectorSource(text))
cloud1 <- Corpus(VectorSource(text1))
cloud2 <- Corpus(VectorSource(text2))
cloud3 <- Corpus(VectorSource(text3))
```


```{r, warning=FALSE, message=FALSE}
removeNumPunct <- function(x) {
  gsub("[^[:alpha:][:space:]]*", "", x)
}
removeAllCaps <- function(x) {
  gsub("\b[A-Z]+\b", "", x)
}


clean_corpus <- function(corpus) {
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(removeAllCaps))
  corpus <- tm_map(corpus, content_transformer(tolower))
  #adding too common and not insightful words to the stopwords list
  corpus <- tm_map(corpus, removeWords, c("syria","syrian","south","sudan","venezuela","venezuelas","venezuelans","venezuelan","year","years","one","two","three","later","since","month","months","didnt","first","second","third","back","even","now","among","may","get","day","yet","say","country","countries","countrys","president","middle","many","still","says", "bus","left","like","want", "sudanese","sudans","refugee", stopwords("en")))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(removeNumPunct))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

suc_clean <- clean_corpus(cloud)

suc_tdm <- TermDocumentMatrix(suc_clean)

suc_m <- as.matrix(suc_tdm)

suc_v <- sort(rowSums(suc_m), decreasing = TRUE)

suc_d <- data.frame(word = names(suc_v), freq = suc_v)
```


```{r, message=FALSE, warning=FALSE}
#purples <- brewer.pal(7, "Purples")
#purples <- purples[-c(1:2)]
set.seed(1234)
wordcloud(
  words = suc_d$word,
  freq = suc_d$freq,scale = c(2, 0.4),
  min.freq = 5,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.3,
  colors = brewer.pal(8, "Dark2")
)
```

```{r}
suc_clean1 <- clean_corpus(cloud1)

suc_tdm1 <- TermDocumentMatrix(suc_clean1)

suc_m1 <- as.matrix(suc_tdm1)

suc_v1 <- sort(rowSums(suc_m1), decreasing = TRUE)

suc_d1 <- data.frame(word = names(suc_v1), freq = suc_v1)
```
```{r, message=FALSE, warning=FALSE}
#purples <- brewer.pal(7, "Purples")
#purples <- purples[-c(1:2)]
set.seed(1234)
wordcloud(
  words = suc_d1$word,
  freq = suc_d1$freq,scale = c(2, 0.4),
  min.freq = 5,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.3,
  colors = brewer.pal(8, "Dark2")
)
```



```{r}
suc_clean2 <- clean_corpus(cloud2)

suc_tdm2 <- TermDocumentMatrix(suc_clean2)

suc_m2 <- as.matrix(suc_tdm2)

suc_v2 <- sort(rowSums(suc_m2), decreasing = TRUE)

suc_d2 <- data.frame(word = names(suc_v2), freq = suc_v2)
```
```{r, message=FALSE, warning=FALSE}
#purples <- brewer.pal(7, "Purples")
#purples <- purples[-c(1:2)]
set.seed(1234)
wordcloud(
  words = suc_d2$word,
  freq = suc_d2$freq,scale = c(2, 0.4),
  min.freq = 5,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.3,
  colors = brewer.pal(8, "Dark2")
)
```


```{r}
suc_clean3 <- clean_corpus(cloud3)

suc_tdm3 <- TermDocumentMatrix(suc_clean3)

suc_m3 <- as.matrix(suc_tdm3)

suc_v3 <- sort(rowSums(suc_m3), decreasing = TRUE)

suc_d3 <- data.frame(word = names(suc_v3), freq = suc_v3)
```
```{r, message=FALSE, warning=FALSE}
#purples <- brewer.pal(7, "Purples")
#purples <- purples[-c(1:2)]
set.seed(1234)
wordcloud(
  words = suc_d3$word,
  freq = suc_d3$freq,scale = c(2, 0.4),
  min.freq = 5,
  max.words = 100,
  random.order = FALSE,
  rot.per = 0.3,
  colors = brewer.pal(8, "Dark2")
)
```

```{r}
d1_syria <- suc_d1 
names(d1_syria)[names(d1_syria) == "freq"] <- "freq_syria"
d2_venezuela <- suc_d2 
names(d2_venezuela)[names(d2_venezuela) == "freq"] <- "freq_venezuela"
d3_ssudan <- suc_d3 
names(d3_ssudan)[names(d3_ssudan) == "freq"] <- "freq_ssudan"
```

Cleaning up and prepping the dataframe for visualization
```{r, warning=FALSE, message=FALSE}
#top common words by frequency are in combined3 
Combined2 <- inner_join(d2_venezuela, d3_ssudan, by = "word") 
Combined3<-inner_join(d1_syria, Combined2, by = "word")
```

```{r, warning=FALSE, message=FALSE}
Combined5<- Combined3 %>% 
top_n(5, freq_syria) %>%
  arrange(desc(freq_syria))
```
```{r, warning=FALSE, message=FALSE}
Combined6<- Combined3 %>% 
top_n(5, freq_venezuela) %>%
  arrange(desc(freq_venezuela))
```
```{r, warning=FALSE, message=FALSE}
Combined7<- Combined3 %>% 
top_n(5, freq_ssudan) %>%
  arrange(desc(freq_ssudan))
```

append these multiple dataframes into combined 4
```{r}
Combined4 <- rbind(Combined5,Combined6, Combined7)
```

```{r, warning=FALSE, message=FALSE}
names(Combined4)[names(Combined4) == "freq_ssudan"] <- "South Sudan"
names(Combined4)[names(Combined4) == "freq_syria"] <- "Syria"
names(Combined4)[names(Combined4) == "freq_venezuela"] <- "Venezuela"
```

```{r}
library(reshape2)
test1<- reshape2::melt(Combined4, id.vars="word")
names(test1)[names(test1) == "value"] <- "count"
names(test1)[names(test1) == "variable"] <- "Country"
```

```{r}
emotion_dtm <- test1
emotion_dtm %>%
  group_by(word, Country) 
```

```{r}
library(unikn)
emotion_dtm %>% ggplot(aes(x = word, y = count)) +
  geom_col(aes(fill = Country)) +
  facet_wrap(~ Country) +
  coord_flip()+
theme(panel.background = element_rect(fill = "white",
                                        colour = "black",
                                        size = 0.5, 
                                        linetype = "solid"),
        panel.grid.major.y = element_blank(),
        panel.grid.major.x = element_line(size = .1, 
                                          color = "black",
                                          linetype = "dashed")) + scale_fill_manual(values = usecol(pal_unikn_pair, n = 3, alpha = 0.8)) +
  labs(x = "Common Words",
       y = "Frequency",
       title = "Frequent Common Words by Country")
```

Barplots by Frequency
```{r}
barplot(d1_syria[1:20,]$freq, las = 2, names.arg = d1_syria[1:20,]$word,
        col ="lightblue", main ="Most frequent words: Syria ",
        ylab = "Word frequencies", cex.names=0.5)
```
```{r}
barplot(d2_venezuela[1:20,]$freq, las = 2, names.arg = d2_venezuela[1:20,]$word,
        col ="lightblue", main ="Most frequent words: Venezuela ",
        ylab = "Word frequencies", cex.names=0.5)
```
```{r}
barplot(d3_ssudan[1:20,]$freq, las = 2, names.arg = d3_ssudan[1:20,]$word,
        col ="lightblue", main ="Most frequent words: South Sudan ",
        ylab = "Word frequencies", cex.names=0.5)
```

#Bigrams and topic modeling (explorations)
Inputing files in list format
```{r, include=FALSE}
library(readr)
migration<- read_csv("~/Desktop/dataviz/final_project/all_dataviz.csv")
syria1<- read_csv("~/Desktop/dataviz/final_project/syria.csv")
venezuela1<- read_csv("~/Desktop/dataviz/final_project/venezuela.csv")
ssudan1<- read_csv("~/Desktop/dataviz/final_project/ssudan.csv")
```


```{r, echo=FALSE, warning=FALSE,message=FALSE}
#Checking with bi-grams
# BigramTokenizer <-function(x) NGramTokenizer(x,
# tdm <- TermDocumentMatrix(suc_clean,
#                         control = list(wordLengths=c(1,Inf), tokenize = BigramTokenizer, weighting = weightTfIdf))
# inspect(tdm)
# suc_m1 <- as.matrix(tdm)
# suc_v1 <- sort(rowSums(suc_m1), decreasing = TRUE)
# suc_d1 <- data.frame(word = names(suc_v1), freq = suc_v1)
# 
# purples <- brewer.pal(7, "Purples")
# purples <- purples[-c(1:2)]
# set.seed(1234)
# wordcloud(
#   words = suc_d1$word,
#   freq = suc_d1$freq,
#   min.freq = 10,
#   max.words = 100,
#   random.order = FALSE,
#   rot.per = 0.35,
#   colors = purples
# )

```



```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(devtools)
library(stm)
# Process texts:
# The function textProcessor() does a bunch of preprocessing. It lowecases all letters,
# removes stop words, removes numbers, removes punctuation, and stems the text.
processed1 <- textProcessor(documents=migration$text, metadata = migration)
```

```{r, message=FALSE, echo=FALSE,warning=FALSE}
# After we preprocess the text, we perform several additional operations to organize the data
# to be processed by the STM. We save the output in the obect "out":
out <- prepDocuments(processed1$documents, processed1$vocab, processed1$meta)
```

```{r, message=FALSE, echo=FALSE,warning=FALSE}
# Estimate the Structural Topic Model:
# Using the preprocessed data in  "out", we estimate a structural topic model with 10 topics
# we can change the number of topics by setting K to be a different number
migration_stm <- stm(documents = out$documents, vocab = out$vocab, K = 8,init.type = "LDA")
```

```{r, message=FALSE, echo=FALSE,warning=FALSE}
labelTopics(migration_stm, n=15)
```
```{r}
plot(migration_stm, type = "summary", xlim = c(0, 1), n=10)
```

